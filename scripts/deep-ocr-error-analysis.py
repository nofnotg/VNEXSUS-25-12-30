#!/usr/bin/env python3
"""
OCR ë‚ ì§œ ì˜¤ë¥˜ ì •ë°€ ë¶„ì„
- ì¢Œí‘œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
- ì¸ì‹ ì˜¤ë¥˜ vs ì˜ëª» ì¸ì‹ êµ¬ë¶„
- ì˜ë£Œ ì´ë²¤íŠ¸ ë‚´ìš© ë¶„ì„
"""
import json
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple
from dataclasses import dataclass

@dataclass
class Block:
    page: int
    text: str
    x: float
    y: float
    width: float
    height: float
    confidence: float = 0.0

@dataclass
class ErrorAnalysis:
    error_date: str
    error_type: str  # "ì¸ì‹ì˜¤ë¥˜" | "ì˜ëª»ì¸ì‹" | "ìˆ«ìë°°ì—´ì˜¤ì¸"
    found_in_blocks: bool
    block_index: int = -1
    surrounding_text: List[str] = None
    coordinates: Dict = None
    spatial_context: str = ""
    medical_keywords: List[str] = None
    reasoning: str = ""

def extract_dates_from_text(text: str) -> Set[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
    patterns = [
        r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',
        r'(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼',
    ]
    dates = set()
    for pattern in patterns:
        for match in re.finditer(pattern, text):
            year, month, day = match.groups()
            normalized = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            dates.add(normalized)
    return dates

def load_offline_ocr(json_path: Path) -> List[Block]:
    """offline_ocr.jsonì—ì„œ ë¸”ë¡ ë¡œë“œ"""
    with open(json_path, encoding='utf-8') as f:
        data = json.load(f)

    blocks = []
    if 'blocks' in data and isinstance(data['blocks'], list):
        for b in data['blocks']:
            bbox = b.get('bbox', {})
            blocks.append(Block(
                page=bbox.get('page', 0),
                text=b.get('text', ''),
                x=bbox.get('x', 0),
                y=bbox.get('y', 0),
                width=bbox.get('width', 0),
                height=bbox.get('height', 0),
                confidence=b.get('confidence', 0)
            ))

    return blocks

def find_block_with_date(blocks: List[Block], date_str: str) -> Tuple[int, Block]:
    """ë‚ ì§œë¥¼ í¬í•¨í•œ ë¸”ë¡ ì°¾ê¸°"""
    # ì •í™•í•œ ë‚ ì§œ ë¬¸ìì—´ ê²€ìƒ‰
    for i, block in enumerate(blocks):
        if date_str in block.text:
            return i, block

    # ë‚ ì§œ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
    year, month, day = date_str.split('-')
    patterns = [
        f"{year}-{month}-{day}",
        f"{year}.{month}.{day}",
        f"{year}/{month}/{day}",
        f"{year}ë…„{month}ì›”{day}ì¼",
        f"{year}ë…„ {month}ì›” {day}ì¼",
        f"{int(year)}.{int(month)}.{int(day)}",
    ]

    for i, block in enumerate(blocks):
        for pattern in patterns:
            if pattern in block.text:
                return i, block

    return -1, None

def get_surrounding_blocks(blocks: List[Block], index: int, window: int = 5) -> List[Tuple[int, Block]]:
    """ì£¼ë³€ ë¸”ë¡ ì¶”ì¶œ (Â±window)"""
    start = max(0, index - window)
    end = min(len(blocks), index + window + 1)
    return [(i, blocks[i]) for i in range(start, end)]

def calculate_spatial_distance(b1: Block, b2: Block) -> float:
    """ë‘ ë¸”ë¡ ê°„ ê³µê°„ ê±°ë¦¬"""
    if b1.page != b2.page:
        return float('inf')

    # Center points
    x1 = b1.x + b1.width / 2
    y1 = b1.y + b1.height / 2
    x2 = b2.x + b2.width / 2
    y2 = b2.y + b2.height / 2

    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5

def find_spatially_close_blocks(blocks: List[Block], target: Block, max_distance: float = 100) -> List[Tuple[int, Block, float]]:
    """ì¢Œí‘œ ê¸°ë°˜ìœ¼ë¡œ ê°€ê¹Œìš´ ë¸”ë¡ ì°¾ê¸°"""
    close_blocks = []
    for i, block in enumerate(blocks):
        if block.page == target.page:
            distance = calculate_spatial_distance(target, block)
            if distance < max_distance:
                close_blocks.append((i, block, distance))

    return sorted(close_blocks, key=lambda x: x[2])

def extract_medical_keywords(text: str) -> List[str]:
    """ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = [
        # ì§„ë‹¨/ì²˜ì¹˜
        'ì§„ë‹¨', 'ê²€ì‚¬', 'ìˆ˜ìˆ ', 'ì²˜ë°©', 'íˆ¬ì•½', 'ì¹˜ë£Œ', 'ì…ì›', 'í‡´ì›',
        'ì™¸ë˜', 'ì‘ê¸‰', 'ìˆ˜í˜ˆ', 'ì£¼ì‚¬', 'ì´¬ì˜', 'íŒë…',
        # ì§„ë£Œê³¼
        'ë‚´ê³¼', 'ì™¸ê³¼', 'ì •í˜•ì™¸ê³¼', 'ì‹ ê²½ì™¸ê³¼', 'ì‚°ë¶€ì¸ê³¼', 'ì†Œì•„ê³¼', 'ì´ë¹„ì¸í›„ê³¼',
        # ê²€ì‚¬ëª…
        'CT', 'MRI', 'X-ray', 'ì´ˆìŒíŒŒ', 'í˜ˆì•¡ê²€ì‚¬', 'ì†Œë³€ê²€ì‚¬',
        # ì˜ë£Œí–‰ìœ„
        'ì²˜ë°©ì „', 'ì§„ë£Œê¸°ë¡', 'ì†Œê²¬ì„œ', 'ì˜ê²¬ì„œ', 'ì§„ë‹¨ì„œ',
        # ë‚ ì§œ ê´€ë ¨
        'ì´ˆì§„', 'ì¬ì§„', 'ë‚´ì›', 'ë°©ë¬¸', 'ê²½ê³¼', 'ì¶”ì ',
    ]

    found = []
    for keyword in keywords:
        if keyword in text:
            found.append(keyword)

    return found

def analyze_date_sequence(blocks: List[Block], target_date: str, window: int = 10) -> Dict:
    """ë‚ ì§œ ì‹œí€€ìŠ¤ ë¶„ì„ - ì •ë°°ì—´ ì²´í¬"""
    all_dates = []

    for i, block in enumerate(blocks):
        dates = extract_dates_from_text(block.text)
        for date in dates:
            try:
                year = int(date.split('-')[0])
                if 1900 <= year <= 2100:  # Valid year range
                    all_dates.append((i, date, block.page, block.y))
            except:
                pass

    # Sort by page and Y position
    all_dates.sort(key=lambda x: (x[2], x[3]))

    # Find target date position
    target_index = -1
    for i, (_, date, _, _) in enumerate(all_dates):
        if date == target_date:
            target_index = i
            break

    if target_index == -1:
        return {
            'found_in_sequence': False,
            'total_dates': len(all_dates),
            'before': [],
            'after': []
        }

    # Extract surrounding dates
    start = max(0, target_index - window)
    end = min(len(all_dates), target_index + window + 1)

    before = all_dates[start:target_index]
    after = all_dates[target_index+1:end]

    return {
        'found_in_sequence': True,
        'total_dates': len(all_dates),
        'position': target_index,
        'before': [(date, page) for _, date, page, _ in before[-5:]],
        'after': [(date, page) for _, date, page, _ in after[:5]],
        'target': (target_date, all_dates[target_index][2])
    }

def classify_error_type(error_date: str, blocks: List[Block], baseline_dates: Set[str]) -> ErrorAnalysis:
    """ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜"""

    # 1. ë¸”ë¡ì—ì„œ ë‚ ì§œ ì°¾ê¸°
    block_idx, found_block = find_block_with_date(blocks, error_date)

    if block_idx == -1:
        # ë‚ ì§œê°€ ë¸”ë¡ì— ì—†ìŒ - baseline ì˜¤ë¥˜ ê°€ëŠ¥ì„±
        return ErrorAnalysis(
            error_date=error_date,
            error_type="baseline_error",
            found_in_blocks=False,
            reasoning="ì´ ë‚ ì§œëŠ” OCR ë¸”ë¡ì—ì„œ ë°œê²¬ë˜ì§€ ì•ŠìŒ. Baseline íŒŒì¼ì˜ ì˜¤ë¥˜ì´ê±°ë‚˜ OCRì´ ëˆ„ë½í•œ ê²ƒì¼ ìˆ˜ ìˆìŒ."
        )

    # 2. ì£¼ë³€ ë¸”ë¡ ì¶”ì¶œ
    surrounding = get_surrounding_blocks(blocks, block_idx, window=5)
    surrounding_text = [f"[{i}] {b.text[:100]}" for i, b in surrounding]

    # 3. ì¢Œí‘œ ê¸°ë°˜ ê°€ê¹Œìš´ ë¸”ë¡ ì°¾ê¸°
    spatially_close = find_spatially_close_blocks(blocks, found_block, max_distance=50)

    # 4. ë‚ ì§œ ì‹œí€€ìŠ¤ ë¶„ì„
    sequence = analyze_date_sequence(blocks, error_date)

    # 5. ì˜ë£Œ í‚¤ì›Œë“œ ì¶”ì¶œ
    context_text = ' '.join([b.text for _, b in surrounding])
    medical_keywords = extract_medical_keywords(context_text)

    # 6. ì˜¤ë¥˜ ìœ í˜• íŒë‹¨
    year = int(error_date.split('-')[0])
    month = int(error_date.split('-')[1])
    day = int(error_date.split('-')[2])

    # ë¶ˆê°€ëŠ¥í•œ ë‚ ì§œ ì²´í¬
    if month < 1 or month > 12:
        error_type = "ì¸ì‹ì˜¤ë¥˜"
        reasoning = f"ì›”ì´ {month}ë¡œ ì¸ì‹ë¨ - OCRì´ ìˆ«ìë¥¼ ì˜ëª» ì½ì—ˆì„ ê°€ëŠ¥ì„±. "
    elif day < 1 or day > 31:
        error_type = "ì¸ì‹ì˜¤ë¥˜"
        reasoning = f"ì¼ì´ {day}ë¡œ ì¸ì‹ë¨ - OCRì´ ìˆ«ìë¥¼ ì˜ëª» ì½ì—ˆì„ ê°€ëŠ¥ì„±. "
    elif year > 2026 or year < 1950:
        error_type = "ì¸ì‹ì˜¤ë¥˜"
        reasoning = f"ì—°ë„ê°€ {year}ë¡œ ì¸ì‹ë¨ - OCRì´ ìˆ«ìë¥¼ ì˜ëª» ì½ì—ˆì„ ê°€ëŠ¥ì„±. "

        # ì‹œí€€ìŠ¤ ì²´í¬
        if sequence['found_in_sequence']:
            reasoning += f"ë‚ ì§œ ì‹œí€€ìŠ¤ ìƒ ì „í›„: {sequence['before'][-2:]} â†’ {error_date} â†’ {sequence['after'][:2]}. "
    else:
        # ë¯¸ë˜ ë‚ ì§œì¸ ê²½ìš°
        from datetime import datetime
        try:
            date_obj = datetime.strptime(error_date, "%Y-%m-%d")
            if date_obj > datetime.now():
                error_type = "ì˜ëª»ì¸ì‹"
                reasoning = f"ë¯¸ë˜ ë‚ ì§œë¡œ ì¸ì‹ë¨. "

                # ì˜ë£Œ í‚¤ì›Œë“œ ì²´í¬
                if medical_keywords:
                    reasoning += f"ì£¼ë³€ì—ì„œ ë°œê²¬ëœ ì˜ë£Œ í‚¤ì›Œë“œ: {', '.join(medical_keywords[:5])}. "
                    reasoning += "ì˜ë£Œ ì´ë²¤íŠ¸ì˜ ì˜ˆì •/ê³„íš ë‚ ì§œì¼ ê°€ëŠ¥ì„±ë„ ìˆìŒ. "
                else:
                    reasoning += "ì˜ë£Œ í‚¤ì›Œë“œê°€ ê±°ì˜ ì—†ì–´ ë‚ ì§œê°€ ì•„ë‹Œ ìˆ«ì ë°°ì—´ì„ ë‚ ì§œë¡œ ì˜¤ì¸í–ˆì„ ê°€ëŠ¥ì„±. "
            else:
                error_type = "ì˜ëª»ì¸ì‹"
                reasoning = "ê³¼ê±° ë‚ ì§œì´ì§€ë§Œ Baselineì— ëˆ„ë½ë¨. "
        except:
            error_type = "ìˆ«ìë°°ì—´ì˜¤ì¸"
            reasoning = "ë‚ ì§œ í˜•ì‹ì´ì§€ë§Œ ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ. "

    return ErrorAnalysis(
        error_date=error_date,
        error_type=error_type,
        found_in_blocks=True,
        block_index=block_idx,
        surrounding_text=surrounding_text,
        coordinates={
            'page': found_block.page,
            'x': found_block.x,
            'y': found_block.y,
            'width': found_block.width,
            'height': found_block.height,
        },
        spatial_context=f"Page {found_block.page}, ì¢Œí‘œ ({found_block.x:.1f}, {found_block.y:.1f})",
        medical_keywords=medical_keywords,
        reasoning=reasoning
    )

def main():
    # Load validation results
    results_path = Path("/home/user/VNEXSUS-25-12-30/outputs/quick_validation_28.json")
    with open(results_path) as f:
        results = json.load(f)

    # Find error cases (í•˜ ë“±ê¸‰ or ì˜¤ë¥˜ ë‚ ì§œ ìˆëŠ” ì¼€ì´ìŠ¤)
    error_cases = [
        r for r in results
        if r['grade'] == 'í•˜' or len(r['impossible_dates']) > 0 or len(r['future_dates']) > 0
    ]

    print("=" * 100)
    print("OCR ë‚ ì§œ ì˜¤ë¥˜ ì •ë°€ ë¶„ì„")
    print("=" * 100)
    print()
    print(f"ë¶„ì„ ëŒ€ìƒ: {len(error_cases)}ê°œ ì¼€ì´ìŠ¤")
    print()

    all_analyses = []

    for case in error_cases:
        case_name = case['case_name']
        print(f"\n{'='*80}")
        print(f"ì¼€ì´ìŠ¤: {case_name}")
        print(f"{'='*80}")
        print(f"ë“±ê¸‰: {case['grade']} | ì •í™•ë„: {case['accuracy']:.1f}%")
        print(f"Baseline: {case['baseline_dates']}ê°œ | OCR: {case['ocr_dates']}ê°œ | ëˆ„ë½: {case['missing']}ê°œ")

        # Load OCR file
        cases_json = Path("/home/user/VNEXSUS-25-12-30/validation_cases_28.json")
        with open(cases_json) as f:
            cases = json.load(f)

        case_info = next((c for c in cases if c['name'] == case_name), None)
        if not case_info:
            print("  âš ï¸  ì¼€ì´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            continue

        ocr_file = Path(case_info['ocr_file'])
        if not ocr_file.exists():
            print("  âš ï¸  OCR íŒŒì¼ ì—†ìŒ")
            continue

        # Load blocks
        blocks = load_offline_ocr(ocr_file)
        print(f"\nğŸ“¦ ì´ ë¸”ë¡ ìˆ˜: {len(blocks)}")

        # Load baseline
        baseline_file = Path(case_info['baseline_file'])
        baseline_text = baseline_file.read_text(encoding='utf-8')
        baseline_dates = extract_dates_from_text(baseline_text)

        # Analyze error dates
        error_dates = case['impossible_dates'] + case['future_dates']
        if not error_dates:
            # ëˆ„ë½ëœ ë‚ ì§œ ì¤‘ ì¼ë¶€ ë¶„ì„
            error_dates = case['missing_sample'][:3]

        print(f"\nğŸ” ì˜¤ë¥˜ ë‚ ì§œ ë¶„ì„: {len(error_dates)}ê°œ")

        for error_date in error_dates:
            analysis = classify_error_type(error_date, blocks, baseline_dates)
            all_analyses.append({
                'case': case_name,
                'analysis': analysis
            })

            print(f"\n  ğŸ“… {error_date}")
            print(f"     ìœ í˜•: {analysis.error_type}")
            print(f"     OCR ë¸”ë¡ ë°œê²¬: {'âœ… ì˜ˆ' if analysis.found_in_blocks else 'âŒ ì•„ë‹ˆì˜¤'}")

            if analysis.found_in_blocks:
                print(f"     ìœ„ì¹˜: {analysis.spatial_context}")
                print(f"     ë¸”ë¡ ì¸ë±ìŠ¤: {analysis.block_index}")

                if analysis.medical_keywords:
                    print(f"     ì˜ë£Œ í‚¤ì›Œë“œ: {', '.join(analysis.medical_keywords[:5])}")

                if analysis.surrounding_text:
                    print(f"     ì£¼ë³€ í…ìŠ¤íŠ¸ (ìƒ˜í”Œ):")
                    for text in analysis.surrounding_text[:3]:
                        print(f"       {text}")

            print(f"     ì¶”ë¡ : {analysis.reasoning}")

    # Summary
    print("\n" + "=" * 100)
    print("ë¶„ì„ ìš”ì•½")
    print("=" * 100)

    type_counts = {}
    for item in all_analyses:
        error_type = item['analysis'].error_type
        type_counts[error_type] = type_counts.get(error_type, 0) + 1

    print(f"\nì´ ë¶„ì„ ì˜¤ë¥˜: {len(all_analyses)}ê°œ")
    print(f"\nìœ í˜•ë³„ ë¶„í¬:")
    for error_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):
        print(f"  {error_type}: {count}ê°œ ({count/len(all_analyses)*100:.1f}%)")

    # Save results
    output_path = Path("/home/user/VNEXSUS-25-12-30/outputs/deep_ocr_analysis.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump([
            {
                'case': item['case'],
                'error_date': item['analysis'].error_date,
                'error_type': item['analysis'].error_type,
                'found_in_blocks': item['analysis'].found_in_blocks,
                'spatial_context': item['analysis'].spatial_context,
                'medical_keywords': item['analysis'].medical_keywords,
                'reasoning': item['analysis'].reasoning,
            }
            for item in all_analyses
        ], f, indent=2, ensure_ascii=False)

    print(f"\nâœ… ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")

    # Insights
    print("\n" + "=" * 100)
    print("100% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ê°œì„  ì¸ì‚¬ì´íŠ¸")
    print("=" * 100)

    baseline_errors = sum(1 for item in all_analyses if not item['analysis'].found_in_blocks)
    ocr_errors = len(all_analyses) - baseline_errors

    print(f"\n1. Baseline ì˜¤ë¥˜ ({baseline_errors}ê°œ):")
    print("   - ì˜¤ë¥˜ ë‚ ì§œê°€ OCR ë¸”ë¡ì— ì—†ìŒ")
    print("   - ê°œì„ : Baseline íŒŒì¼ ì¬ê²€ì¦ í•„ìš”")
    print()

    print(f"2. OCR ì¸ì‹ ì˜¤ë¥˜ ({ocr_errors}ê°œ):")
    recognition_errors = sum(1 for item in all_analyses if item['analysis'].error_type == 'ì¸ì‹ì˜¤ë¥˜')
    misidentified = sum(1 for item in all_analyses if item['analysis'].error_type == 'ì˜ëª»ì¸ì‹')
    number_array = sum(1 for item in all_analyses if item['analysis'].error_type == 'ìˆ«ìë°°ì—´ì˜¤ì¸')

    if recognition_errors > 0:
        print(f"   a) ì¸ì‹ ì˜¤ë¥˜ ({recognition_errors}ê°œ):")
        print("      - OCRì´ ì‹¤ì œ ë‚ ì§œë¥¼ ì˜ëª» ì½ìŒ")
        print("      - ê°œì„ : ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦ + ì£¼ë³€ ë‚ ì§œ ì‹œí€€ìŠ¤ ê¸°ë°˜ ë³´ì •")

    if misidentified > 0:
        print(f"   b) ì˜ëª» ì¸ì‹ ({misidentified}ê°œ):")
        print("      - ë‚ ì§œê°€ ì•„ë‹Œ ê²ƒì„ ë‚ ì§œë¡œ ì¸ì‹")
        print("      - ê°œì„ : ì˜ë£Œ ì´ë²¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§")

    if number_array > 0:
        print(f"   c) ìˆ«ì ë°°ì—´ ì˜¤ì¸ ({number_array}ê°œ):")
        print("      - ìˆ«ì ë°°ì—´ì„ ë‚ ì§œë¡œ ì˜¤ì¸")
        print("      - ê°œì„ : ë‚ ì§œ íŒ¨í„´ ê°•í™” + ì£¼ë³€ í…ìŠ¤íŠ¸ ë¶„ì„")

    print()

if __name__ == "__main__":
    main()
