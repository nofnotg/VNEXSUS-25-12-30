#!/usr/bin/env python3
"""
ì „ì²´ ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì¢…í•© ë¶„ì„
"""
import json
import re
from pathlib import Path
from typing import Set, Dict, List
from collections import defaultdict

def extract_dates_comprehensive(text: str) -> Dict[str, str]:
    """ëª¨ë“  ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ"""
    patterns = [
        (r'(\d{4})[-](\d{1,2})[-](\d{1,2})', '-'),
        (r'(\d{4})[.](\d{1,2})[.](\d{1,2})', '.'),
        (r'(\d{4})[/](\d{1,2})[/](\d{1,2})', '/'),
        (r'(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', 'ë…„'),
    ]

    dates = {}
    for pattern, sep in patterns:
        for match in re.finditer(pattern, text):
            year, month, day = match.groups()
            normalized = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            original = match.group(0)
            if normalized not in dates:
                dates[normalized] = original

    return dates

def analyze_case(case_name: str, baseline_file: Path, ocr_file: Path) -> Dict:
    """ì¼€ì´ìŠ¤ ë¶„ì„"""
    # Baseline
    baseline_text = baseline_file.read_text(encoding='utf-8')
    baseline_dates = extract_dates_comprehensive(baseline_text)

    # OCR
    with open(ocr_file, encoding='utf-8') as f:
        data = json.load(f)

    ocr_text = '\n'.join([block.get('text', '') for block in data['blocks']])
    ocr_dates = extract_dates_comprehensive(ocr_text)

    # ë¹„êµ
    baseline_only = set(baseline_dates.keys()) - set(ocr_dates.keys())
    ocr_only = set(ocr_dates.keys()) - set(baseline_dates.keys())
    common = set(baseline_dates.keys()) & set(ocr_dates.keys())

    # ëˆ„ë½ëœ ë‚ ì§œì˜ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸°
    missing_contexts = []
    for date in sorted(baseline_only):
        original = baseline_dates[date]
        for i, line in enumerate(baseline_text.split('\n'), 1):
            if original in line:
                missing_contexts.append({
                    'date': date,
                    'original': original,
                    'line_num': i,
                    'context': line.strip()[:150]
                })
                break

    return {
        'case_name': case_name,
        'baseline_count': len(baseline_dates),
        'ocr_count': len(ocr_dates),
        'matched': len(common),
        'ocr_missed': len(baseline_only),
        'ocr_extra': len(ocr_only),
        'accuracy': len(common) / len(baseline_dates) * 100 if baseline_dates else 100,
        'missing_contexts': missing_contexts
    }

# Load cases
cases_json = Path("/home/user/VNEXSUS-25-12-30/validation_cases_28.json")
with open(cases_json) as f:
    cases = json.load(f)

# Load validation results
results_path = Path("/home/user/VNEXSUS-25-12-30/outputs/quick_validation_28.json")
with open(results_path) as f:
    results = json.load(f)

# í•˜ ë“±ê¸‰ + ì¤‘ ë“±ê¸‰ ì¼€ì´ìŠ¤ ë¶„ì„
error_cases = [r for r in results if r['grade'] in ['í•˜', 'ì¤‘']]

print("=" * 100)
print("ì˜¤ë¥˜ ì¼€ì´ìŠ¤ ì¢…í•© ë¶„ì„ (í•˜/ì¤‘ ë“±ê¸‰)")
print("=" * 100)
print(f"\në¶„ì„ ëŒ€ìƒ: {len(error_cases)}ê°œ ì¼€ì´ìŠ¤")
print()

all_analyses = []
all_missing_contexts = []

for case_result in error_cases:
    case_name = case_result['case_name']

    # Find case info
    case_info = next((c for c in cases if c['name'] == case_name), None)
    if not case_info:
        continue

    baseline_file = Path(case_info['baseline_file'])
    ocr_file = Path(case_info['ocr_file'])

    if not baseline_file.exists() or not ocr_file.exists():
        continue

    analysis = analyze_case(case_name, baseline_file, ocr_file)
    all_analyses.append(analysis)

    print(f"\n{'='*80}")
    print(f"{case_name} (ë“±ê¸‰: {case_result['grade']})")
    print(f"{'='*80}")
    print(f"Baseline: {analysis['baseline_count']}ê°œ | OCR: {analysis['ocr_count']}ê°œ | ë§¤ì¹­: {analysis['matched']}ê°œ")
    print(f"OCR ëˆ„ë½: {analysis['ocr_missed']}ê°œ | OCR ì¶”ê°€: {analysis['ocr_extra']}ê°œ")
    print(f"ì •í™•ë„: {analysis['accuracy']:.1f}%")

    if analysis['missing_contexts']:
        print(f"\nðŸ“ OCRì´ ëˆ„ë½í•œ ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸:")
        for ctx in analysis['missing_contexts'][:5]:
            print(f"  â€¢ {ctx['date']} (Line {ctx['line_num']})")
            print(f"    {ctx['context']}")
            all_missing_contexts.append({
                'case': case_name,
                **ctx
            })

# íŒ¨í„´ ë¶„ì„
print("\n" + "=" * 100)
print("ëˆ„ë½ íŒ¨í„´ ë¶„ì„")
print("=" * 100)

# í‚¤ì›Œë“œ ë¹ˆë„
keyword_freq = defaultdict(int)
for ctx in all_missing_contexts:
    context = ctx['context']
    keywords = ['ë³´í—˜ê¸°ê°„', 'ê³„ì•½', 'ê°€ìž…', 'ë³´í—˜', 'ë‹´ë³´', 'ê¸°ê°„', '~', 'â‘ ', 'â‘¡']
    for keyword in keywords:
        if keyword in context:
            keyword_freq[keyword] += 1

print(f"\nðŸ“Š ëˆ„ë½ëœ ë‚ ì§œê°€ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ë§¥ í‚¤ì›Œë“œ:")
for keyword, count in sorted(keyword_freq.items(), key=lambda x: -x[1])[:10]:
    print(f"  {keyword}: {count}íšŒ")

# ë‚ ì§œ ë²”ìœ„ ë¶„ì„
future_dates = [ctx for ctx in all_missing_contexts if int(ctx['date'].split('-')[0]) >= 2026]
past_dates = [ctx for ctx in all_missing_contexts if int(ctx['date'].split('-')[0]) < 2024]

print(f"\nðŸ“… ëˆ„ë½ëœ ë‚ ì§œ ì‹œê°„ëŒ€ ë¶„ì„:")
print(f"  ë¯¸ëž˜ ë‚ ì§œ (2026+): {len(future_dates)}ê°œ")
print(f"  ê³¼ê±° ë‚ ì§œ (<2024): {len(past_dates)}ê°œ")
print(f"  ìµœê·¼ ë‚ ì§œ (2024-2025): {len(all_missing_contexts) - len(future_dates) - len(past_dates)}ê°œ")

if future_dates:
    print(f"\n  ë¯¸ëž˜ ë‚ ì§œ ìƒ˜í”Œ:")
    for ctx in future_dates[:3]:
        print(f"    {ctx['date']} - {ctx['context'][:100]}")

# ì¢…í•© í†µê³„
print("\n" + "=" * 100)
print("ì¢…í•© í†µê³„")
print("=" * 100)

total_baseline = sum(a['baseline_count'] for a in all_analyses)
total_ocr = sum(a['ocr_count'] for a in all_analyses)
total_matched = sum(a['matched'] for a in all_analyses)
total_missed = sum(a['ocr_missed'] for a in all_analyses)
total_extra = sum(a['ocr_extra'] for a in all_analyses)

print(f"\nì´ Baseline ë‚ ì§œ: {total_baseline}ê°œ")
print(f"ì´ OCR ë‚ ì§œ: {total_ocr}ê°œ")
print(f"ì´ ë§¤ì¹­: {total_matched}ê°œ")
print(f"ì´ OCR ëˆ„ë½: {total_missed}ê°œ ({total_missed/total_baseline*100:.1f}%)")
print(f"ì´ OCR ì¶”ê°€: {total_extra}ê°œ")

avg_accuracy = sum(a['accuracy'] for a in all_analyses) / len(all_analyses)
print(f"\ní‰ê·  ì •í™•ë„: {avg_accuracy:.1f}%")

# Save results
output_path = Path("/home/user/VNEXSUS-25-12-30/outputs/comprehensive_error_analysis.json")
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump({
        'summary': {
            'total_cases': len(all_analyses),
            'total_baseline': total_baseline,
            'total_ocr': total_ocr,
            'total_matched': total_matched,
            'total_missed': total_missed,
            'total_extra': total_extra,
            'average_accuracy': avg_accuracy
        },
        'cases': all_analyses,
        'missing_contexts': all_missing_contexts,
        'patterns': {
            'keyword_frequency': dict(keyword_freq),
            'future_dates_count': len(future_dates),
            'past_dates_count': len(past_dates)
        }
    }, f, indent=2, ensure_ascii=False)

print(f"\nâœ… ìƒì„¸ ë¶„ì„ ì €ìž¥: {output_path}")

print("\n" + "=" * 100)
print("100% ì •í™•ë„ ë‹¬ì„± ì¸ì‚¬ì´íŠ¸")
print("=" * 100)
print()
print("ðŸ” í•µì‹¬ ë°œê²¬:")
print()
print("1. 'ì˜¤ë¥˜'ë¡œ ë¶„ë¥˜ëœ ë‚ ì§œë“¤ì€ ì‹¤ì œë¡œ OCR ì¸ì‹ ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ **OCR ëˆ„ë½ (False Negative)**")
print("   - OCRì´ ë¬¸ì„œì— ìžˆëŠ” ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨")
print("   - Baseline ë³´ê³ ì„œëŠ” ì‚¬ëžŒì´ ê²€í† í•˜ì—¬ ìž‘ì„±í•œ ì •í™•í•œ ë‚ ì§œ")
print()
print("2. ëˆ„ë½ íŒ¨í„´:")
print(f"   - ì£¼ë¡œ 'ë³´í—˜ê¸°ê°„', 'ê³„ì•½' ê´€ë ¨ ë¬¸ë§¥ì—ì„œ ë°œìƒ")
print(f"   - í‘œ êµ¬ì¡°ë‚˜ íŠ¹ìˆ˜ ë ˆì´ì•„ì›ƒì—ì„œ OCR ì‹¤íŒ¨ ê°€ëŠ¥ì„±")
print(f"   - ë¯¸ëž˜ ë‚ ì§œ({len(future_dates)}ê°œ)ëŠ” ë³´í—˜ ë§Œê¸°ì¼ ë“± ì¤‘ìš” ì •ë³´")
print()
print("3. OCRì´ ì¶”ê°€ë¡œ ê²€ì¶œí•œ ë‚ ì§œ({total_extra}ê°œ):")
print("   - ë¬¸ì„œ ì „ì²´ì—ì„œ ëª¨ë“  ë‚ ì§œ í˜•ì‹ì„ ì¶”ì¶œ")
print("   - í•˜ì§€ë§Œ Baselineì—ì„œ ì¤‘ìš”í•œ ë‚ ì§œë§Œ ì„ ë³„í•œ ê²ƒê³¼ ëŒ€ì¡°")
print()
print("ðŸ’¡ ê°œì„  ì „ëžµ:")
print()
print("A. ë‹¨ê¸° (Quick Win - 1ì£¼):")
print("   âœ… ë‚ ì§œ ì •ê·œí™”: ë‹¤ì–‘í•œ í¬ë§· í†µì¼ (ì /í•˜ì´í”ˆ/ìŠ¬ëž˜ì‹œ)")
print("   âœ… ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦: ë¶ˆê°€ëŠ¥í•œ ë‚ ì§œ í•„í„°ë§")
print("   âœ… LLM í›„ì²˜ë¦¬: OCR ê²°ê³¼ì—ì„œ ëˆ„ë½ëœ ì¤‘ìš” ë‚ ì§œ ë³µì›")
print()
print("B. ì¤‘ê¸° (Medium Win - 1ê°œì›”):")
print("   ðŸ”„ ë¬¸ë§¥ ê¸°ë°˜ ë‚ ì§œ ì¶”ì¶œ:")
print("      - 'ë³´í—˜ê¸°ê°„', 'ê³„ì•½ì¼', 'ê°€ìž…ì¼' ë“± í‚¤ì›Œë“œ ì£¼ë³€ ë‚ ì§œ ìš°ì„  ì¶”ì¶œ")
print("      - í‘œ êµ¬ì¡° íŒŒì‹± ê°•í™”")
print("   ðŸ”„ ë‚ ì§œ ì‹œí€€ìŠ¤ ê²€ì¦:")
print("      - ë³´í—˜ê¸°ê°„ ì‹œìž‘~ì¢…ë£Œ ë‚ ì§œ ìŒ ì¸ì‹")
print("      - ì—°ëŒ€ìˆœ ì •ë ¬ í›„ ì´ìƒì¹˜ íƒì§€")
print()
print("C. ìž¥ê¸° (Long-term - 3ê°œì›”):")
print("   ðŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼:")
print("      - OCR + LLM ë‚ ì§œ ì¶”ì¶œ ì¡°í•©")
print("      - LLMì´ ì˜ë£Œë³´í—˜ ë„ë©”ì¸ ì§€ì‹ í™œìš©í•˜ì—¬ ëˆ„ë½ ë‚ ì§œ ì¶”ë¡ ")
print("      - ì˜ˆ: 'ë³´í—˜ê¸°ê°„ 30ë…„' â†’ ì‹œìž‘ì¼ + 30ë…„ = ì¢…ë£Œì¼ ê³„ì‚°")
print("   ðŸš€ ë¬¸ì„œ êµ¬ì¡° ì´í•´:")
print("      - í‘œ, ì„¹ì…˜, í—¤ë” ì¸ì‹ ê°•í™”")
print("      - ì¤‘ìš” ì •ë³´ ì˜ì—­ (ê³„ì•½ì‚¬í•­, ì²­êµ¬ì‚¬í•­) ìš°ì„  ì²˜ë¦¬")
print()
